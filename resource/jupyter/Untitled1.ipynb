{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c9430bbd-9e73-48ae-bca1-e944c173a2d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onepl_lsrm_cont_missing(data,\n",
    "\n",
    "                            ndim,\n",
    "                            niter,\n",
    "                            nburn,\n",
    "                            nthin,\n",
    "                            nprint,\n",
    "\n",
    "                            jump_beta,\n",
    "                            jump_theta,\n",
    "                            jump_gamma,\n",
    "                            jump_z,\n",
    "                            jump_w,\n",
    "\n",
    "                            pr_mean_beta,\n",
    "                            pr_sd_beta,\n",
    "                            pr_a_th_sigma,\n",
    "                            pr_b_th_sigma,\n",
    "                            pr_mean_theta,\n",
    "\n",
    "                            pr_a_sigma,\n",
    "                            pr_b_sigma,\n",
    "                            pr_mean_gamma,\n",
    "                            pr_sd_gamma,\n",
    "\n",
    "                            missing):\n",
    "    ##############################################################################\n",
    "\n",
    "    def update_function(beta, theta, gamma):\n",
    "        return (- pow((data[k, i] - beta[i] - theta[k] + gamma * dist[k, i]), 2) /\n",
    "                (2 * pow(pr_sd, 2))\n",
    "                )\n",
    "\n",
    "    def rejection_algorithm(ratio):\n",
    "        if ratio > 0.0:\n",
    "            accept = 1\n",
    "        else:\n",
    "            un = np.random.uniform(1)\n",
    "            if np.log(un) < ratio:\n",
    "                accept = 1\n",
    "            else:\n",
    "                accept = 0\n",
    "\n",
    "        return accept\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    nsample = data.shape[0]\n",
    "    nitem = data.shape[1]\n",
    "\n",
    "    pr_mean_z = pr_mean_w = 0.0\n",
    "    pr_sd_z = pr_sd_w = pr_sd = pr_sd_theta = 1.0\n",
    "\n",
    "    oldbeta = np.random.uniform(size=nitem)\n",
    "    oldtheta = np.random.uniform(size=nsample)\n",
    "    oldz = np.random.uniform(size=(nsample, ndim))\n",
    "    oldw = np.random.uniform(size=(nitem, ndim))\n",
    "\n",
    "    # oldbeta = np.random.uniform(nitem)\n",
    "    # oldtheta = np.random.uniform(nsample)\n",
    "    # oldz = np.random.rand(nsample, ndim)\n",
    "    # oldw = np.random.rand(nitem, ndim)\n",
    "\n",
    "    newbeta = oldbeta = oldbeta * 4.0 - 2.0\n",
    "    newtheta = oldtheta = oldtheta * 4.0 - 2.0\n",
    "    newz = oldz = oldz * 2.0 - 1.0\n",
    "    neww = oldw = oldw * 2.0 - 1.0\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    oldgamma = newgamma = 1  # gamma1 = log(gamma)\n",
    "\n",
    "    samp_beta = np.zeros(shape=((niter - nburn) // nthin, nitem))\n",
    "    samp_theta = np.zeros(shape=((niter - nburn) // nthin, nsample))\n",
    "\n",
    "    samp_z = np.zeros(shape=((niter - nburn) // nthin, nsample, ndim))\n",
    "    samp_w = np.zeros(shape=((niter - nburn) // nthin, nitem, ndim))\n",
    "\n",
    "    samp_sd_theta = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_sd = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_mle = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_gamma = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "\n",
    "    accept_beta = np.zeros(shape=nitem, )\n",
    "    accept_theta = np.zeros(shape=nsample, )\n",
    "    accept_z = np.zeros(shape=nsample, )\n",
    "    accept_w = np.zeros(shape=nitem, )\n",
    "\n",
    "    accept_gamma = 0\n",
    "    accept = 0\n",
    "    count = 0\n",
    "\n",
    "    dist = np.zeros(shape=(nsample, nitem), )\n",
    "\n",
    "    old_dist_k = np.zeros(nitem, )\n",
    "    new_dist_k = np.zeros(nitem, )\n",
    "    old_dist_i = np.zeros(nsample, )\n",
    "    new_dist_i = np.zeros(nsample, )\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    for iter in range(niter):\n",
    "        # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "        dist = np.where(True, 0, dist)  # 매 이터레이션마다 거리 매트릭스를 0으로 리셋\n",
    "\n",
    "        for i in range(nitem):\n",
    "            for k in range(nsample):\n",
    "                dist_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                    dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "        # beta update\n",
    "        for i in range(nitem):\n",
    "            # TODO 컬럼부터 갱신하는 이유가 있는건가?\n",
    "\n",
    "            newbeta[i] = np.random.normal(oldbeta[i], jump_beta, 1)\n",
    "            old_like_beta = new_like_beta = 0.0\n",
    "\n",
    "            for k in range(nsample):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_beta += update_function(newbeta, oldtheta, oldgamma)\n",
    "                    old_like_beta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "            num = (new_like_beta +\n",
    "                   scipy.stats.norm.logpdf(newbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "            den = (old_like_beta +\n",
    "                   scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "\n",
    "            if accept == 1:\n",
    "                oldbeta[i] = newbeta[i]\n",
    "                accept_beta[i] += 1.0 / (niter * 1.0)\n",
    "\n",
    "            else:\n",
    "                newbeta[i] = oldbeta[i]\n",
    "\n",
    "        # theta update\n",
    "        for k in range(nsample):\n",
    "            newtheta[k] = np.random.normal(oldtheta[k], jump_theta, 1)\n",
    "            old_like_theta = new_like_theta = 0.0\n",
    "\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_theta += update_function(oldbeta, newtheta, oldgamma)\n",
    "                    old_like_theta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "            num = (new_like_theta +\n",
    "                   scipy.stats.norm.logpdf(newtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "            den = (old_like_theta +\n",
    "                   scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                oldtheta[k] = newtheta[k]\n",
    "                accept_theta[k] += 1.0 / (niter * 1.0)\n",
    "            else:\n",
    "                newtheta[k] = oldtheta[k]\n",
    "\n",
    "        # gamma(log(gamma)) update\n",
    "\n",
    "        newgamma = np.random.lognormal(log(oldgamma), jump_gamma, 1)\n",
    "        old_like_gamma = new_like_gamma = 0.0\n",
    "\n",
    "        for k in range(nsample):\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_gamma += update_function(oldbeta, newtheta, newgamma)\n",
    "                    old_like_gamma += update_function(oldbeta, newtheta, oldgamma)\n",
    "\n",
    "        num = (new_like_gamma +\n",
    "               scipy.stats.lognorm.logpdf(oldgamma, s=jump_gamma, loc=0, scale=exp(log(newgamma))) +\n",
    "               scipy.stats.lognorm.logpdf(newgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "               )\n",
    "        den = (old_like_gamma +\n",
    "               scipy.stats.lognorm.logpdf(newgamma, s=jump_gamma, loc=0, scale=exp(log(oldgamma))) +\n",
    "               scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "               )\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "        if accept == 1:\n",
    "            oldgamma = newgamma\n",
    "            accept_gamma += 1.0 / (niter * 1.0)\n",
    "        else:\n",
    "            newgamma = oldgamma\n",
    "\n",
    "        # zj update\n",
    "\n",
    "        for k in range(nsample):\n",
    "            for j in range(ndim):\n",
    "                newz[k, j] = np.random.normal(oldz[k, j], jump_z, 1)\n",
    "            old_like_z = new_like_z = 0.0\n",
    "\n",
    "            # calculate distance of oldw and newz\n",
    "\n",
    "            for i in range(nitem):\n",
    "                dist_old_temp = dist_new_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_new_temp += pow((newz[k, j] - oldw[i, j]), 2.0)\n",
    "                    dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                new_dist_k[i] = sqrt(dist_new_temp)\n",
    "                old_dist_k[i] = sqrt(dist_old_temp)\n",
    "\n",
    "            # calculate likelihood\n",
    "\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_k[i]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "                    old_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_k[i]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "\n",
    "            num = den = 0.0\n",
    "\n",
    "            for j in range(ndim):\n",
    "                num += scipy.stats.norm.logpdf(newz[k, j], pr_mean_z, pr_sd_z)\n",
    "                den += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "\n",
    "            # Rprintf(\"%.3f %.3f %.3f %.3f\\n\", num, den, new_like_z, old_like_z)\n",
    "            # arma::dvec newzz = dmvnorm(newz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "            # arma::dvec oldzz = dmvnorm(oldz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "\n",
    "            num += new_like_z\n",
    "            den += old_like_z\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                for j in range(ndim):\n",
    "                    oldz[k, j] = newz[k, j]\n",
    "                accept_z[k] += 1.0 / (niter * 1.0)\n",
    "\n",
    "            else:\n",
    "                for j in range(ndim):\n",
    "                    newz[k, j] = oldz[k, j]\n",
    "\n",
    "                #  wi update\n",
    "        for i in range(nitem):\n",
    "            for j in range(ndim):\n",
    "                neww[i, j] = np.random.normal(oldw[i, j], jump_w, 1)\n",
    "            old_like_w = new_like_w = 0.0\n",
    "\n",
    "            # calculate distance of neww and oldz\n",
    "\n",
    "            for k in range(nsample):\n",
    "                dist_old_temp = dist_new_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_new_temp += pow((oldz[k, j] - neww[i, j]), 2.0)  # TODO: Why Old - New?\n",
    "                    dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                new_dist_i[k] = sqrt(dist_new_temp)\n",
    "                old_dist_i[k] = sqrt(dist_old_temp)\n",
    "\n",
    "            # calculate likelihood\n",
    "\n",
    "            for k in range(nsample):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_i[k]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "                    old_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_i[k]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "\n",
    "            num = den = 0.0\n",
    "\n",
    "            for j in range(ndim):\n",
    "                num += scipy.stats.norm.logpdf(neww[i, j], pr_mean_w, pr_sd_w)\n",
    "                den += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "\n",
    "            num += new_like_w\n",
    "            den += old_like_w\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                for j in range(ndim):\n",
    "                    oldw[i, j] = neww[i, j]\n",
    "                accept_w[i] += 1.0 / (niter * 1.0)\n",
    "            else:\n",
    "                for j in range(ndim):\n",
    "                    neww[i, j] = oldw[i, j]\n",
    "\n",
    "        # sigma_theta update with gibbs\n",
    "\n",
    "        post_a_th_sigma = pr_a_th_sigma * 2 + nsample\n",
    "        post_b_th_sigma = pr_b_th_sigma\n",
    "\n",
    "        for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "            post_b_th_sigma += pow((oldtheta[j] - pr_mean_theta), 2.0)\n",
    "        pr_sd_theta = sqrt(2 * post_b_th_sigma * (1.0 / np.random.chisquare(post_a_th_sigma)))\n",
    "\n",
    "        # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "        dist = np.where(True, 0, dist)\n",
    "\n",
    "        for i in range(nitem):\n",
    "            for k in range(nsample):\n",
    "                dist_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "        # sigma update with gibbs\n",
    "\n",
    "        post_a_sigma = pr_a_sigma * 2 + nsample * nitem\n",
    "        post_b_sigma = pr_b_sigma\n",
    "\n",
    "        for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "            for i in range(nitem):\n",
    "                post_b_sigma += pow((data[j, i] - oldbeta[i] - oldtheta[j] + oldgamma * dist[j, i]), 2.0) / 2\n",
    "\n",
    "        pr_sd = sqrt(2 * post_b_sigma * (1.0 / np.random.chisquare(post_a_sigma)))\n",
    "\n",
    "        # burn, thin\n",
    "\n",
    "        if iter >= nburn and iter % nthin == 0:\n",
    "            for i in range(nitem):\n",
    "                samp_beta[count, i] = oldbeta[i]\n",
    "            for k in range(nsample):\n",
    "                samp_theta[count, k] = oldtheta[k]\n",
    "            for i in range(nitem):\n",
    "                for j in range(ndim):\n",
    "                    samp_w[count, i, j] = oldw[i, j]\n",
    "            for k in range(nsample):\n",
    "                for j in range(ndim):\n",
    "                    samp_z[count, k, j] = oldz[k, j]\n",
    "\n",
    "            samp_gamma[count] = oldgamma\n",
    "            samp_sd_theta[count] = pr_sd_theta\n",
    "            samp_sd[count] = pr_sd\n",
    "\n",
    "            mle = 0.0\n",
    "\n",
    "            for i in range(nitem):\n",
    "                mle += scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta)\n",
    "            for k in range(nsample):\n",
    "                mle += scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta)\n",
    "            for i in range(nitem):\n",
    "                for j in range(ndim):\n",
    "                    mle += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "            for k in range(nsample):\n",
    "                for j in range(ndim):\n",
    "                    mle += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "            for k in range(nsample):\n",
    "                for i in range(nitem):\n",
    "                    mle += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * dist[k, i]), 2)\n",
    "                            / (2 * pow(pr_sd, 2)))\n",
    "\n",
    "            mle += scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "            samp_mle[count] = mle\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if iter % nprint == 0:\n",
    "            print(\"Iteration: \", iter)\n",
    "            print(\"count\", count)\n",
    "            for i in range(nitem):\n",
    "                print(\"nitem: \", i, \" with \", oldbeta[i])\n",
    "            print(\"oldgamma: \", oldgamma, \"     pr_sd_theta: \", pr_sd_theta)\n",
    "\n",
    "    return {\"beta\": samp_beta,\n",
    "            \"theta\": samp_theta,\n",
    "            \"z\": samp_z,\n",
    "            \"w\": samp_w,\n",
    "            \"gamma\": samp_gamma,\n",
    "            \"sigma_theta\": samp_sd_theta,\n",
    "            \"sigma\": samp_sd,\n",
    "            \"map\": samp_mle,\n",
    "            \"accept_beta\": accept_beta,\n",
    "            \"accept_theta\": accept_theta,\n",
    "            \"accept_z\": accept_z,\n",
    "            \"accept_w\": accept_w,\n",
    "            \"accept_gamma\": accept_gamma}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5e778dce-206f-440c-92c0-684ed3cbef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = logit_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f7ab2cea-68b4-43dd-b8df-ef4af5d8e760",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-219-db07af29f152>:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-219-db07af29f152>:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-219-db07af29f152>:159: RuntimeWarning: invalid value encountered in subtract\n",
      "  ratio = num - den\n",
      "<ipython-input-219-db07af29f152>:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-219-db07af29f152>:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "count 1\n",
      "nitem:  0  with  0.4371162397109419\n",
      "nitem:  1  with  -1.7421795238232058\n",
      "nitem:  2  with  -1.304654071773013\n",
      "nitem:  3  with  -2.262637449943894\n",
      "oldgamma:  1      pr_sd_theta:  2.2000015713941834\n",
      "Iteration:  10\n",
      "count 2\n",
      "nitem:  0  with  1.6128293866124275\n",
      "nitem:  1  with  -1.0276045691568965\n",
      "nitem:  2  with  -1.3506734173458765\n",
      "nitem:  3  with  -0.8422314356728288\n",
      "oldgamma:  1      pr_sd_theta:  5.028935065226805\n",
      "Iteration:  20\n",
      "count 3\n",
      "nitem:  0  with  0.4093960858106128\n",
      "nitem:  1  with  -2.148886925129096\n",
      "nitem:  2  with  -0.5584097745759579\n",
      "nitem:  3  with  -0.7100183999680388\n",
      "oldgamma:  1      pr_sd_theta:  5.998000346915622\n",
      "Iteration:  30\n",
      "count 4\n",
      "nitem:  0  with  0.4523679954773709\n",
      "nitem:  1  with  -2.3047036366116243\n",
      "nitem:  2  with  -2.9024035725653348\n",
      "nitem:  3  with  -0.7405812081982937\n",
      "oldgamma:  1      pr_sd_theta:  7.663005879532199\n",
      "Iteration:  40\n",
      "count 5\n",
      "nitem:  0  with  0.5102541230586677\n",
      "nitem:  1  with  -2.175875165982859\n",
      "nitem:  2  with  -3.950221141862005\n",
      "nitem:  3  with  -0.6248544877971158\n",
      "oldgamma:  1      pr_sd_theta:  8.356929824956515\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-219-db07af29f152>, line 347)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-219-db07af29f152>\"\u001b[1;36m, line \u001b[1;32m347\u001b[0m\n\u001b[1;33m    return {\"beta\": samp_beta,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "\n",
    "def update_function(beta, theta, gamma):\n",
    "    return (- pow((data[k, i] - beta[i] - theta[k] + gamma * dist[k, i]), 2) /\n",
    "            (2 * pow(pr_sd, 2))\n",
    "            )\n",
    "\n",
    "def rejection_algorithm(ratio):\n",
    "    if ratio > 0.0:\n",
    "        accept = 1\n",
    "    else:\n",
    "        un = np.random.uniform(1)\n",
    "        if np.log(un) < ratio:\n",
    "            accept = 1\n",
    "        else:\n",
    "            accept = 0\n",
    "\n",
    "    return accept\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "nsample = data.shape[0]\n",
    "nitem = data.shape[1]\n",
    "\n",
    "pr_mean_z = pr_mean_w = 0.0\n",
    "pr_sd_z = pr_sd_w = pr_sd = pr_sd_theta = 1.0\n",
    "\n",
    "oldbeta = np.random.uniform(size=nitem)\n",
    "oldtheta = np.random.uniform(size=nsample)\n",
    "oldz = np.random.uniform(size=(nsample, ndim))\n",
    "oldw = np.random.uniform(size=(nitem, ndim))\n",
    "\n",
    "# oldbeta = np.random.uniform(nitem)\n",
    "# oldtheta = np.random.uniform(nsample)\n",
    "# oldz = np.random.rand(nsample, ndim)\n",
    "# oldw = np.random.rand(nitem, ndim)\n",
    "\n",
    "oldbeta = oldbeta * 4.0 - 2.0\n",
    "oldtheta = oldtheta * 4.0 - 2.0\n",
    "oldz = oldz * 2.0 - 1.0\n",
    "oldw = oldw * 2.0 - 1.0\n",
    "\n",
    "newbeta = oldbeta \n",
    "newtheta = oldtheta\n",
    "newz = oldz \n",
    "neww = oldw \n",
    "\n",
    "##############################################################################\n",
    "\n",
    "oldgamma = 1\n",
    "newgamma = 1  # gamma1 = log(gamma)\n",
    "\n",
    "samp_beta = np.zeros(shape=((niter - nburn) // nthin, nitem))\n",
    "samp_theta = np.zeros(shape=((niter - nburn) // nthin, nsample))\n",
    "\n",
    "samp_z = np.zeros(shape=((niter - nburn) // nthin, nsample, ndim))\n",
    "samp_w = np.zeros(shape=((niter - nburn) // nthin, nitem, ndim))\n",
    "\n",
    "samp_sd_theta = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "samp_sd = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "samp_mle = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "samp_gamma = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "\n",
    "accept_beta = np.zeros(shape=nitem, )\n",
    "accept_theta = np.zeros(shape=nsample, )\n",
    "accept_z = np.zeros(shape=nsample, )\n",
    "accept_w = np.zeros(shape=nitem, )\n",
    "\n",
    "accept_gamma = 0\n",
    "accept = 0\n",
    "count = 0\n",
    "\n",
    "dist = np.zeros(shape=(nsample, nitem), )\n",
    "\n",
    "old_dist_k = np.zeros(nitem, )\n",
    "new_dist_k = np.zeros(nitem, )\n",
    "old_dist_i = np.zeros(nsample, )\n",
    "new_dist_i = np.zeros(nsample, )\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "for iter in range(niter):\n",
    "    # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "    dist = np.where(True, 0, dist)  # 매 이터레이션마다 거리 매트릭스를 0으로 리셋\n",
    "\n",
    "    for i in range(nitem):\n",
    "        for k in range(nsample):\n",
    "            dist_temp = 0.0\n",
    "            for j in range(ndim):\n",
    "                dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "    # beta update\n",
    "    for i in range(nitem):\n",
    "        # TODO 컬럼부터 갱신하는 이유가 있는건가?\n",
    "\n",
    "        newbeta[i] = np.random.normal(oldbeta[i], jump_beta, 1)\n",
    "        old_like_beta = new_like_beta = 0.0\n",
    "\n",
    "        for k in range(nsample):\n",
    "            if data[k, i] != missing:\n",
    "                new_like_beta += update_function(newbeta, oldtheta, oldgamma)\n",
    "                old_like_beta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "        num = (new_like_beta +\n",
    "               scipy.stats.norm.logpdf(newbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "        den = (old_like_beta +\n",
    "               scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "\n",
    "        if accept == 1:\n",
    "            oldbeta[i] = newbeta[i]\n",
    "            accept_beta[i] += 1.0 / (niter * 1.0)\n",
    "\n",
    "        else:\n",
    "            newbeta[i] = oldbeta[i]\n",
    "\n",
    "    # theta update\n",
    "    for k in range(nsample):\n",
    "        newtheta[k] = np.random.normal(oldtheta[k], jump_theta, 1)\n",
    "        old_like_theta = new_like_theta = 0.0\n",
    "\n",
    "        for i in range(nitem):\n",
    "            if data[k, i] != missing:\n",
    "                new_like_theta += update_function(oldbeta, newtheta, oldgamma)\n",
    "                old_like_theta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "        num = (new_like_theta +\n",
    "               scipy.stats.norm.logpdf(newtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "        den = (old_like_theta +\n",
    "               scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "        if accept == 1:\n",
    "            oldtheta[k] = newtheta[k]\n",
    "            accept_theta[k] += 1.0 / (niter * 1.0)\n",
    "        else:\n",
    "            newtheta[k] = oldtheta[k]\n",
    "\n",
    "    # gamma(log(gamma)) update\n",
    "\n",
    "    newgamma = np.random.lognormal(log(oldgamma), jump_gamma, 1)\n",
    "    old_like_gamma = new_like_gamma = 0.0\n",
    "\n",
    "    for k in range(nsample):\n",
    "        for i in range(nitem):\n",
    "            if data[k, i] != missing:\n",
    "                new_like_gamma += update_function(oldbeta, newtheta, newgamma)\n",
    "                old_like_gamma += update_function(oldbeta, newtheta, oldgamma)\n",
    "\n",
    "    num = (new_like_gamma +\n",
    "           scipy.stats.lognorm.logpdf(oldgamma, s=jump_gamma, loc=0, scale=exp(log(newgamma))) +\n",
    "           scipy.stats.lognorm.logpdf(newgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "           )\n",
    "    den = (old_like_gamma +\n",
    "           scipy.stats.lognorm.logpdf(newgamma, s=jump_gamma, loc=0, scale=exp(log(oldgamma))) +\n",
    "           scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "           )\n",
    "    ratio = num - den\n",
    "\n",
    "    accept = rejection_algorithm(ratio)\n",
    "\n",
    "    if accept == 1:\n",
    "        oldgamma = newgamma\n",
    "        accept_gamma += 1.0 / (niter * 1.0)\n",
    "    else:\n",
    "        newgamma = oldgamma\n",
    "\n",
    "    # zj update\n",
    "\n",
    "    for k in range(nsample):\n",
    "        for j in range(ndim):\n",
    "            newz[k, j] = np.random.normal(oldz[k, j], jump_z, 1)\n",
    "        old_like_z = new_like_z = 0.0\n",
    "\n",
    "        # calculate distance of oldw and newz\n",
    "\n",
    "        for i in range(nitem):\n",
    "            dist_old_temp = dist_new_temp = 0.0\n",
    "            \n",
    "            for j in range(ndim):\n",
    "                dist_new_temp += pow((newz[k, j] - oldw[i, j]), 2.0)\n",
    "                dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "            new_dist_k[i] = sqrt(dist_new_temp)\n",
    "            old_dist_k[i] = sqrt(dist_old_temp)\n",
    "\n",
    "        # calculate likelihood\n",
    "\n",
    "        for i in range(nitem):\n",
    "            if data[k, i] != missing:\n",
    "                new_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_k[i]), 2) /\n",
    "                               (2 * pow(pr_sd, 2))\n",
    "                               )\n",
    "                old_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_k[i]), 2) /\n",
    "                               (2 * pow(pr_sd, 2))\n",
    "                               )\n",
    "\n",
    "        num = den = 0.0\n",
    "\n",
    "        for j in range(ndim):\n",
    "            num += scipy.stats.norm.logpdf(newz[k, j], pr_mean_z, pr_sd_z)\n",
    "            den += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "\n",
    "        # Rprintf(\"%.3f %.3f %.3f %.3f\\n\", num, den, new_like_z, old_like_z)\n",
    "        # arma::dvec newzz = dmvnorm(newz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "        # arma::dvec oldzz = dmvnorm(oldz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "\n",
    "        num += new_like_z\n",
    "        den += old_like_z\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "        if accept == 1:\n",
    "            for j in range(ndim):\n",
    "                oldz[k, j] = newz[k, j]\n",
    "            accept_z[k] += 1.0 / (niter * 1.0)\n",
    "\n",
    "        else:\n",
    "            for j in range(ndim):\n",
    "                newz[k, j] = oldz[k, j]\n",
    "\n",
    "            #  wi update\n",
    "    for i in range(nitem):\n",
    "        for j in range(ndim):\n",
    "            neww[i, j] = np.random.normal(oldw[i, j], jump_w, 1)\n",
    "        old_like_w = new_like_w = 0.0\n",
    "\n",
    "        # calculate distance of neww and oldz\n",
    "\n",
    "        for k in range(nsample):\n",
    "            dist_old_temp = dist_new_temp = 0.0\n",
    "            for j in range(ndim):\n",
    "                dist_new_temp += pow((oldz[k, j] - neww[i, j]), 2.0)  # TODO: Why Old - New?\n",
    "                dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "            new_dist_i[k] = sqrt(dist_new_temp)\n",
    "            old_dist_i[k] = sqrt(dist_old_temp)\n",
    "\n",
    "        # calculate likelihood\n",
    "\n",
    "        for k in range(nsample):\n",
    "            if data[k, i] != missing:\n",
    "                new_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_i[k]), 2) /\n",
    "                               (2 * pow(pr_sd, 2))\n",
    "                               )\n",
    "                old_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_i[k]), 2) /\n",
    "                               (2 * pow(pr_sd, 2))\n",
    "                               )\n",
    "\n",
    "        num = den = 0.0\n",
    "\n",
    "        for j in range(ndim):\n",
    "            num += scipy.stats.norm.logpdf(neww[i, j], pr_mean_w, pr_sd_w)\n",
    "            den += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "\n",
    "        num += new_like_w\n",
    "        den += old_like_w\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "        if accept == 1:\n",
    "            for j in range(ndim):\n",
    "                oldw[i, j] = neww[i, j]\n",
    "            accept_w[i] += 1.0 / (niter * 1.0)\n",
    "        else:\n",
    "            for j in range(ndim):\n",
    "                neww[i, j] = oldw[i, j]\n",
    "\n",
    "    # sigma_theta update with gibbs\n",
    "\n",
    "    post_a_th_sigma = pr_a_th_sigma * 2 + nsample\n",
    "    post_b_th_sigma = pr_b_th_sigma\n",
    "\n",
    "    for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "        post_b_th_sigma += pow((oldtheta[j] - pr_mean_theta), 2.0)\n",
    "    pr_sd_theta = sqrt(2 * post_b_th_sigma * (1.0 / np.random.chisquare(post_a_th_sigma)))\n",
    "\n",
    "    # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "    dist = np.where(True, 0, dist)\n",
    "\n",
    "    for i in range(nitem):\n",
    "        for k in range(nsample):\n",
    "            dist_temp = 0.0\n",
    "            for j in range(ndim):\n",
    "                dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "            dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "    # sigma update with gibbs\n",
    "\n",
    "    post_a_sigma = pr_a_sigma * 2 + nsample * nitem\n",
    "    post_b_sigma = pr_b_sigma\n",
    "\n",
    "    for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "        for i in range(nitem):\n",
    "            post_b_sigma += pow((data[j, i] - oldbeta[i] - oldtheta[j] + oldgamma * dist[j, i]), 2.0) / 2\n",
    "\n",
    "    pr_sd = sqrt(2 * post_b_sigma * (1.0 / np.random.chisquare(post_a_sigma)))\n",
    "\n",
    "    # burn, thin\n",
    "\n",
    "    if iter >= nburn and iter % nthin == 0:\n",
    "        for i in range(nitem):\n",
    "            samp_beta[count, i] = oldbeta[i]\n",
    "        for k in range(nsample):\n",
    "            samp_theta[count, k] = oldtheta[k]\n",
    "        for i in range(nitem):\n",
    "            for j in range(ndim):\n",
    "                samp_w[count, i, j] = oldw[i, j]\n",
    "        for k in range(nsample):\n",
    "            for j in range(ndim):\n",
    "                samp_z[count, k, j] = oldz[k, j]\n",
    "\n",
    "        samp_gamma[count] = oldgamma\n",
    "        samp_sd_theta[count] = pr_sd_theta\n",
    "        samp_sd[count] = pr_sd\n",
    "\n",
    "        mle = 0.0\n",
    "\n",
    "        for i in range(nitem):\n",
    "            mle += scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta)\n",
    "        for k in range(nsample):\n",
    "            mle += scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta)\n",
    "        for i in range(nitem):\n",
    "            for j in range(ndim):\n",
    "                mle += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "        for k in range(nsample):\n",
    "            for j in range(ndim):\n",
    "                mle += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "        for k in range(nsample):\n",
    "            for i in range(nitem):\n",
    "                mle += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * dist[k, i]), 2)\n",
    "                        / (2 * pow(pr_sd, 2)))\n",
    "\n",
    "        mle += scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "        samp_mle[count] = mle\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if iter % nprint == 0:\n",
    "        print(\"Iteration: \", iter)\n",
    "        print(\"count\", count)\n",
    "        for i in range(nitem):\n",
    "            print(\"nitem: \", i, \" with \", oldbeta[i])\n",
    "        print(\"oldgamma: \", oldgamma, \"     pr_sd_theta: \", pr_sd_theta)\n",
    "\n",
    "return {\"beta\": samp_beta,\n",
    "        \"theta\": samp_theta,\n",
    "        \"z\": samp_z,\n",
    "        \"w\": samp_w,\n",
    "        \"gamma\": samp_gamma,\n",
    "        \"sigma_theta\": samp_sd_theta,\n",
    "        \"sigma\": samp_sd,\n",
    "        \"map\": samp_mle,\n",
    "        \"accept_beta\": accept_beta,\n",
    "        \"accept_theta\": accept_theta,\n",
    "        \"accept_z\": accept_z,\n",
    "        \"accept_w\": accept_w,\n",
    "        \"accept_gamma\": accept_gamma}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ae84fcd7-4e3f-4269-a52c-0b876c49659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([inf, inf, inf, inf, inf])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_beta\n",
    "samp_theta\n",
    "samp_z\n",
    "samp_w\n",
    "samp_gamma\n",
    "samp_sd_theta\n",
    "samp_sd\n",
    "samp_mle\n",
    "accept_beta\n",
    "accept_theta\n",
    "accept_z\n",
    "accept_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dad70e78-43f3-439c-b1b7-0907a40058c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77388ee5-8075-4563-931a-c074295e2793",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8cdfad38-3cfa-4fb6-8a98-add84bbe4243",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onepl_lsrm_cont_missing(data,\n",
    "\n",
    "                            ndim,\n",
    "                            niter,\n",
    "                            nburn,\n",
    "                            nthin,\n",
    "                            nprint,\n",
    "\n",
    "                            jump_beta,\n",
    "                            jump_theta,\n",
    "                            jump_gamma,\n",
    "                            jump_z,\n",
    "                            jump_w,\n",
    "\n",
    "                            pr_mean_beta,\n",
    "                            pr_sd_beta,\n",
    "                            pr_a_th_sigma,\n",
    "                            pr_b_th_sigma,\n",
    "                            pr_mean_theta,\n",
    "\n",
    "                            pr_a_sigma,\n",
    "                            pr_b_sigma,\n",
    "                            pr_mean_gamma,\n",
    "                            pr_sd_gamma,\n",
    "\n",
    "                            missing):\n",
    "    ##############################################################################\n",
    "\n",
    "    def update_function(beta, theta, gamma):\n",
    "        return (- pow((data[k, i] - beta[i] - theta[k] + gamma * dist[k, i]), 2) /\n",
    "                (2 * pow(pr_sd, 2))\n",
    "                )\n",
    "\n",
    "    def rejection_algorithm(ratio):\n",
    "        if ratio > 0.0:\n",
    "            accept = 1\n",
    "        else:\n",
    "            un = np.random.uniform(1)\n",
    "            if np.log(un) < ratio:\n",
    "                accept = 1\n",
    "            else:\n",
    "                accept = 0\n",
    "\n",
    "        return accept\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    nsample = data.shape[0]\n",
    "    nitem = data.shape[1]\n",
    "\n",
    "    pr_mean_z = pr_mean_w = 0.0\n",
    "    pr_sd_z = pr_sd_w = pr_sd = pr_sd_theta = 1.0\n",
    "\n",
    "    oldbeta = np.random.uniform(size=nitem)\n",
    "    oldtheta = np.random.uniform(size=nsample)\n",
    "    oldz = np.random.uniform(size=(nsample, ndim))\n",
    "    oldw = np.random.uniform(size=(nitem, ndim))\n",
    "\n",
    "    # oldbeta = np.random.uniform(nitem)\n",
    "    # oldtheta = np.random.uniform(nsample)\n",
    "    # oldz = np.random.rand(nsample, ndim)\n",
    "    # oldw = np.random.rand(nitem, ndim)\n",
    "\n",
    "    newbeta = oldbeta = oldbeta * 4.0 - 2.0\n",
    "    newtheta = oldtheta = oldtheta * 4.0 - 2.0\n",
    "    newz = oldz = oldz * 2.0 - 1.0\n",
    "    neww = oldw = oldw * 2.0 - 1.0\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    oldgamma = newgamma = 1  # gamma1 = log(gamma)\n",
    "\n",
    "    samp_beta = np.zeros(shape=((niter - nburn) // nthin, nitem))\n",
    "    samp_theta = np.zeros(shape=((niter - nburn) // nthin, nsample))\n",
    "\n",
    "    samp_z = np.zeros(shape=((niter - nburn) // nthin, nsample, ndim))\n",
    "    samp_w = np.zeros(shape=((niter - nburn) // nthin, nitem, ndim))\n",
    "\n",
    "    samp_sd_theta = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_sd = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_mle = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "    samp_gamma = np.zeros(shape=(niter - nburn) // nthin, )\n",
    "\n",
    "    accept_beta = np.zeros(shape=nitem, )\n",
    "    accept_theta = np.zeros(shape=nsample, )\n",
    "    accept_z = np.zeros(shape=nsample, )\n",
    "    accept_w = np.zeros(shape=nitem, )\n",
    "\n",
    "    accept_gamma = 0\n",
    "    accept = 0\n",
    "    count = 0\n",
    "\n",
    "    dist = np.zeros(shape=(nsample, nitem), )\n",
    "\n",
    "    old_dist_k = np.zeros(nitem, )\n",
    "    new_dist_k = np.zeros(nitem, )\n",
    "    old_dist_i = np.zeros(nsample, )\n",
    "    new_dist_i = np.zeros(nsample, )\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "    for iter in range(niter):\n",
    "        # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "        dist = np.where(True, 0, dist)  # 매 이터레이션마다 거리 매트릭스를 0으로 리셋\n",
    "\n",
    "        for i in range(nitem):\n",
    "            for k in range(nsample):\n",
    "                dist_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                    dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "        # beta update\n",
    "        for i in range(nitem):\n",
    "            # TODO 컬럼부터 갱신하는 이유가 있는건가?\n",
    "\n",
    "            newbeta[i] = np.random.normal(oldbeta[i], jump_beta, 1)\n",
    "            old_like_beta = new_like_beta = 0.0\n",
    "\n",
    "            for k in range(nsample):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_beta += update_function(newbeta, oldtheta, oldgamma)\n",
    "                    old_like_beta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "            num = (new_like_beta +\n",
    "                   scipy.stats.norm.logpdf(newbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "            den = (old_like_beta +\n",
    "                   scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta))\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                oldbeta[i] = newbeta[i]\n",
    "                accept_beta[i] += 1.0 / (niter * 1.0)\n",
    "\n",
    "            else:\n",
    "                newbeta[i] = oldbeta[i]\n",
    "\n",
    "        # theta update\n",
    "        for k in range(nsample):\n",
    "            newtheta[k] = np.random.normal(oldtheta[k], jump_theta, 1)\n",
    "            old_like_theta = new_like_theta = 0.0\n",
    "\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_theta += update_function(oldbeta, newtheta, oldgamma)\n",
    "                    old_like_theta += update_function(oldbeta, oldtheta, oldgamma)\n",
    "\n",
    "            num = (new_like_theta +\n",
    "                   scipy.stats.norm.logpdf(newtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "            den = (old_like_theta +\n",
    "                   scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta))\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                oldtheta[k] = newtheta[k]\n",
    "                accept_theta[k] += 1.0 / (niter * 1.0)\n",
    "            else:\n",
    "                newtheta[k] = oldtheta[k]\n",
    "\n",
    "        # gamma(log(gamma)) update\n",
    "\n",
    "        newgamma = np.random.lognormal(log(oldgamma), jump_gamma, 1)\n",
    "        old_like_gamma = new_like_gamma = 0.0\n",
    "\n",
    "        for k in range(nsample):\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_gamma += update_function(oldbeta, newtheta, newgamma)\n",
    "                    old_like_gamma += update_function(oldbeta, newtheta, oldgamma)\n",
    "\n",
    "        num = (new_like_gamma +\n",
    "               scipy.stats.lognorm.logpdf(oldgamma, s=jump_gamma, loc=0, scale=exp(log(newgamma))) +\n",
    "               scipy.stats.lognorm.logpdf(newgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "               )\n",
    "        den = (old_like_gamma +\n",
    "               scipy.stats.lognorm.logpdf(newgamma, s=jump_gamma, loc=0, scale=exp(log(oldgamma))) +\n",
    "               scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "               )\n",
    "        ratio = num - den\n",
    "\n",
    "        accept = rejection_algorithm(ratio)\n",
    "\n",
    "        if accept == 1:\n",
    "            oldgamma = newgamma\n",
    "            accept_gamma += 1.0 / (niter * 1.0)\n",
    "        else:\n",
    "            newgamma = oldgamma\n",
    "\n",
    "        # zj update\n",
    "\n",
    "        for k in range(nsample):\n",
    "            for j in range(ndim):\n",
    "                newz[k, j] = np.random.normal(oldz[k, j], jump_z, 1)\n",
    "            old_like_z = new_like_z = 0.0\n",
    "\n",
    "            # calculate distance of oldw and newz\n",
    "\n",
    "            for i in range(nitem):\n",
    "                dist_old_temp = dist_new_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_new_temp += pow((newz[k, j] - oldw[i, j]), 2.0)\n",
    "                    dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                new_dist_k[i] = sqrt(dist_new_temp)\n",
    "                old_dist_k[i] = sqrt(dist_old_temp)\n",
    "\n",
    "            # calculate likelihood\n",
    "\n",
    "            for i in range(nitem):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_k[i]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "                    old_like_z += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_k[i]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "\n",
    "            num = den = 0.0\n",
    "\n",
    "            for j in range(ndim):\n",
    "                num += scipy.stats.norm.logpdf(newz[k, j], pr_mean_z, pr_sd_z)\n",
    "                den += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "\n",
    "            # Rprintf(\"%.3f %.3f %.3f %.3f\\n\", num, den, new_like_z, old_like_z)\n",
    "            # arma::dvec newzz = dmvnorm(newz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "            # arma::dvec oldzz = dmvnorm(oldz.cols(2*j,2*j+1),pr_mean_z,pr_cov_z,TRUE)\n",
    "\n",
    "            num += new_like_z\n",
    "            den += old_like_z\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                for j in range(ndim):\n",
    "                    oldz[k, j] = newz[k, j]\n",
    "                accept_z[k] += 1.0 / (niter * 1.0)\n",
    "\n",
    "            else:\n",
    "                for j in range(ndim):\n",
    "                    newz[k, j] = oldz[k, j]\n",
    "\n",
    "        #  wi update\n",
    "        for i in range(nitem):\n",
    "            for j in range(ndim):\n",
    "                neww[i, j] = np.random.normal(oldw[i, j], jump_w, 1)\n",
    "            old_like_w = new_like_w = 0.0\n",
    "\n",
    "            # calculate distance of neww and oldz\n",
    "\n",
    "            for k in range(nsample):\n",
    "                dist_old_temp = dist_new_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_new_temp += pow((oldz[k, j] - neww[i, j]), 2.0)  # TODO: Why Old - New?\n",
    "                    dist_old_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                new_dist_i[k] = sqrt(dist_new_temp)\n",
    "                old_dist_i[k] = sqrt(dist_old_temp)\n",
    "\n",
    "            # calculate likelihood\n",
    "\n",
    "            for k in range(nsample):\n",
    "                if data[k, i] != missing:\n",
    "                    new_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * new_dist_i[k]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "                    old_like_w += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * old_dist_i[k]), 2) /\n",
    "                                   (2 * pow(pr_sd, 2))\n",
    "                                   )\n",
    "\n",
    "            num = den = 0.0\n",
    "\n",
    "            for j in range(ndim):\n",
    "                num += scipy.stats.norm.logpdf(neww[i, j], pr_mean_w, pr_sd_w)\n",
    "                den += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "\n",
    "            num += new_like_w\n",
    "            den += old_like_w\n",
    "            ratio = num - den\n",
    "\n",
    "            accept = rejection_algorithm(ratio)\n",
    "\n",
    "            if accept == 1:\n",
    "                for j in range(ndim):\n",
    "                    oldw[i, j] = neww[i, j]\n",
    "                accept_w[i] += 1.0 / (niter * 1.0)\n",
    "            else:\n",
    "                for j in range(ndim):\n",
    "                    neww[i, j] = oldw[i, j]\n",
    "\n",
    "        # sigma_theta update with gibbs\n",
    "\n",
    "        post_a_th_sigma = pr_a_th_sigma * 2 + nsample\n",
    "        post_b_th_sigma = pr_b_th_sigma\n",
    "\n",
    "        for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "            post_b_th_sigma += pow((oldtheta[j] - pr_mean_theta), 2.0)\n",
    "        pr_sd_theta = sqrt(2 * post_b_th_sigma * (1.0 / np.random.chisquare(post_a_th_sigma)))\n",
    "\n",
    "        # dist(j,i) is distance of z_j and w_i\n",
    "\n",
    "        dist = np.where(True, 0, dist)\n",
    "\n",
    "        for i in range(nitem):\n",
    "            for k in range(nsample):\n",
    "                dist_temp = 0.0\n",
    "                for j in range(ndim):\n",
    "                    dist_temp += pow((oldz[k, j] - oldw[i, j]), 2.0)\n",
    "                dist[k, i] = sqrt(dist_temp)\n",
    "\n",
    "        # sigma update with gibbs\n",
    "\n",
    "        post_a_sigma = pr_a_sigma * 2 + nsample * nitem\n",
    "        post_b_sigma = pr_b_sigma\n",
    "\n",
    "        for j in range(nsample):  # TODO: 여기 j인데 nsample인거 맞음?\n",
    "            for i in range(nitem):\n",
    "                post_b_sigma += pow((data[j, i] - oldbeta[i] - oldtheta[j] + oldgamma * dist[j, i]), 2.0) / 2\n",
    "\n",
    "        pr_sd = sqrt(2 * post_b_sigma * (1.0 / np.random.chisquare(post_a_sigma)))\n",
    "\n",
    "        # burn, thin\n",
    "\n",
    "        if iter >= nburn and iter % nthin == 0:\n",
    "            for i in range(nitem):\n",
    "                samp_beta[count, i] = oldbeta[i]\n",
    "            for k in range(nsample):\n",
    "                samp_theta[count, k] = oldtheta[k]\n",
    "            for i in range(nitem):\n",
    "                for j in range(ndim):\n",
    "                    samp_w[count, i, j] = oldw[i, j]\n",
    "            for k in range(nsample):\n",
    "                for j in range(ndim):\n",
    "                    samp_z[count, k, j] = oldz[k, j]\n",
    "\n",
    "            samp_gamma[count] = oldgamma\n",
    "            samp_sd_theta[count] = pr_sd_theta\n",
    "            samp_sd[count] = pr_sd\n",
    "\n",
    "            mle = 0.0\n",
    "\n",
    "            for i in range(nitem):\n",
    "                mle += scipy.stats.norm.logpdf(oldbeta[i], pr_mean_beta, pr_sd_beta)\n",
    "            for k in range(nsample):\n",
    "                mle += scipy.stats.norm.logpdf(oldtheta[k], pr_mean_theta, pr_sd_theta)\n",
    "            for i in range(nitem):\n",
    "                for j in range(ndim):\n",
    "                    mle += scipy.stats.norm.logpdf(oldw[i, j], pr_mean_w, pr_sd_w)\n",
    "            for k in range(nsample):\n",
    "                for j in range(ndim):\n",
    "                    mle += scipy.stats.norm.logpdf(oldz[k, j], pr_mean_z, pr_sd_z)\n",
    "            for k in range(nsample):\n",
    "                for i in range(nitem):\n",
    "                    mle += (- pow((data[k, i] - oldbeta[i] - oldtheta[k] + oldgamma * dist[k, i]), 2)\n",
    "                            / (2 * pow(pr_sd, 2)))\n",
    "\n",
    "            mle += scipy.stats.lognorm.logpdf(oldgamma, s=pr_sd_gamma, loc=0, scale=exp(pr_mean_gamma))\n",
    "            samp_mle[count] = mle\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if iter % nprint == 0:\n",
    "            \n",
    "            print(\"Iteration: \", iter)\n",
    "            print(\"count\", count)\n",
    "            for i in range(nitem):\n",
    "                print(\"nitem: \", i, \" with \", oldbeta[i])\n",
    "            print(\"oldgamma: \", oldgamma, \"     pr_sd_theta: \", pr_sd_theta)\n",
    "\n",
    "    return {\"beta\": samp_beta,\n",
    "            \"theta\": samp_theta,\n",
    "            \"z\": samp_z,\n",
    "            \"w\": samp_w,\n",
    "            \"gamma\": samp_gamma,\n",
    "            \"sigma_theta\": samp_sd_theta,\n",
    "            \"sigma\": samp_sd,\n",
    "            \"map\": samp_mle,\n",
    "            \"accept_beta\": accept_beta,\n",
    "            \"accept_theta\": accept_theta,\n",
    "            \"accept_z\": accept_z,\n",
    "            \"accept_w\": accept_w,\n",
    "            \"accept_gamma\": accept_gamma}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "719690dc-6cde-4331-a529-711467d2f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2 #차원?\n",
    "niter = 50\n",
    "nburn = 0\n",
    "nthin = 10 #thining lapse\n",
    "nprint = 10\n",
    "\n",
    "jump_beta = 0.3\n",
    "jump_theta = 1.0\n",
    "jump_w = 0.06\n",
    "jump_z = 0.50\n",
    "jump_gamma = 0.01\n",
    "\n",
    "pr_mean_beta = 0\n",
    "pr_sd_beta = 1\n",
    "pr_mean_theta = 0\n",
    "pr_sd_theta = 1\n",
    "pr_mean_gamma = 0.0\n",
    "pr_sd_gamma = 1.0\n",
    "pr_a_sigma = 0.001\n",
    "pr_b_sigma = 0.001\n",
    "pr_a_th_sigma = 0.001\n",
    "pr_b_th_sigma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8010b6a6-cf39-46e7-907c-b20e2876432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4c8a9a15-ba45-45a6-9167-3d11e243afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-202-9e0e1f85ea6f>:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-202-9e0e1f85ea6f>:156: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-202-9e0e1f85ea6f>:185: RuntimeWarning: invalid value encountered in subtract\n",
      "  ratio = num - den\n",
      "<ipython-input-202-9e0e1f85ea6f>:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-202-9e0e1f85ea6f>:283: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "count 1\n",
      "nitem:  0  with  0.21024175908722068\n",
      "nitem:  1  with  0.5375555482163185\n",
      "nitem:  2  with  1.7722642859101656\n",
      "nitem:  3  with  -1.4539945844630005\n",
      "oldgamma:  1      pr_sd_theta:  2.12076425826342\n",
      "Iteration:  10\n",
      "count 2\n",
      "nitem:  0  with  -0.43245876246675397\n",
      "nitem:  1  with  1.1837119884417662\n",
      "nitem:  2  with  2.8393768287369943\n",
      "nitem:  3  with  -0.4066235007350091\n",
      "oldgamma:  1      pr_sd_theta:  5.041881457776828\n",
      "Iteration:  20\n",
      "count 3\n",
      "nitem:  0  with  -0.0006913948871042075\n",
      "nitem:  1  with  1.329378813781376\n",
      "nitem:  2  with  3.4941805504506935\n",
      "nitem:  3  with  -0.6547614209291794\n",
      "oldgamma:  1      pr_sd_theta:  6.742976009273222\n",
      "Iteration:  30\n",
      "count 4\n",
      "nitem:  0  with  0.6074624867233792\n",
      "nitem:  1  with  0.8726983094641195\n",
      "nitem:  2  with  3.5463431916724186\n",
      "nitem:  3  with  -1.857962801891612\n",
      "oldgamma:  1      pr_sd_theta:  8.911316033326958\n",
      "Iteration:  40\n",
      "count 5\n",
      "nitem:  0  with  0.18422947223644554\n",
      "nitem:  1  with  1.0850438957472408\n",
      "nitem:  2  with  4.304503353547021\n",
      "nitem:  3  with  -0.06374773414559082\n",
      "oldgamma:  1      pr_sd_theta:  9.070454141980935\n"
     ]
    }
   ],
   "source": [
    "output = onepl_lsrm_cont_missing(logit_x,\n",
    "                                 ndim, niter, nburn, nthin, nprint,\n",
    "                                 jump_beta, jump_theta, jump_gamma, jump_z, jump_w,\n",
    "                                 pr_mean_beta, pr_sd_beta, pr_a_th_sigma, pr_b_th_sigma, pr_mean_theta,\n",
    "                                 pr_a_sigma, pr_b_sigma, pr_mean_gamma, pr_sd_gamma,\n",
    "                                 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabfeef-cf0e-4dc8-84b6-c4260270c53f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75330e54-e59b-4f61-a650-71499ebd640c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = datasets.load_iris()[\"target\"]\n",
    "x2 = datasets.load_iris()[\"data\"]\n",
    "\n",
    "data1 = pd.concat([pd.DataFrame(x1), pd.DataFrame(x2)], axis=1)\n",
    "\n",
    "data = np.array(data1)\n",
    "\n",
    "data = np.array(data1)\n",
    "\n",
    "data_word = data[:, 0] # 데이터 인풋. 인풋된 데이터는 data. input 되는 것은 논문 더미들.\n",
    "data = np.delete(data, 0, 1) # 0번째 column은 data_word. 뽑아내고 나머지 데이타 보존.\n",
    "\n",
    "data = np.nan_to_num(data, copy=True, nan=99) # na인 cell은 값을 99로 설정.\n",
    "\n",
    "data_m = data # 데이터 타입 매트릭스로 바뀐 데이터. 파이썬에선 pd.dataframe 사용\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_m = MinMaxScaler().fit_transform(data_m)\n",
    "\n",
    "logit_x = np.log(data_m / (1 - data_m)) # 매트릭스 데이터를 logit化\n",
    "\n",
    "\n",
    "# <editor-fold desc=\"covid_20 parameters\">\n",
    "\n",
    "ndim = 2 #차원?\n",
    "niter = 2000\n",
    "nburn = 0\n",
    "nthin = 5 #thining lapse\n",
    "nprint = 1000\n",
    "\n",
    "\"\"\"\n",
    "모든 topic의 분포는 theta ~ 디리클레 알파를 따름\n",
    "\n",
    "biterm 총체 B에서 biterm b를 뽑으면, 이 biterm이 어느 topic z에 속할지는 z ~ 다항분포 세타\n",
    "\n",
    "topic z의 topic-word 분포는 phi_z ~ 디리클레 베타를 따름\n",
    "topic = z, word = w. z가 정해졌을 때 이로부터 각 단어가 가지는 확률을 모아서 set으로 한것이 phi_z.\n",
    "\n",
    "골라진 topic에 대응하는 topic-word 분포로부터 단어 2개가 골라질 확률은 w_i, w_j ~ 다항(phi_z)를 따름\n",
    "\"\"\"\n",
    "\n",
    "jump_beta = 0.3\n",
    "jump_theta = 1.0\n",
    "jump_w = 0.06\n",
    "jump_z = 0.50\n",
    "jump_gamma = 0.01\n",
    "\n",
    "pr_mean_beta = 0\n",
    "pr_sd_beta = 1\n",
    "pr_mean_theta = 0\n",
    "pr_sd_theta = 1\n",
    "pr_mean_gamma = 0.0\n",
    "pr_sd_gamma = 1.0\n",
    "pr_a_sigma = 0.001\n",
    "pr_b_sigma = 0.001\n",
    "pr_a_th_sigma = 0.001\n",
    "pr_b_th_sigma = 0.001\n",
    "# </editor-fold> #TODO 뭐지? 패러미터 스태티스틱?\n",
    "#TODO 뭐지? 패러미터 스태티스틱?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e63630be-ab8d-443c-afb9-a921c945d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num-den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0dc9249c-f997-48d5-bd9f-e2b298eaac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-137-020e0c0e9c6a>:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-137-020e0c0e9c6a>:155: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-137-020e0c0e9c6a>:184: RuntimeWarning: invalid value encountered in subtract\n",
      "  ratio = num - den\n",
      "<ipython-input-137-020e0c0e9c6a>:234: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n",
      "<ipython-input-137-020e0c0e9c6a>:282: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ratio = num - den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "count 1\n",
      "nitem:  0  with  1.8620152940472952\n",
      "nitem:  1  with  1.7135343004546215\n",
      "nitem:  2  with  -0.928583526886758\n",
      "nitem:  3  with  -2.4909841654941762\n",
      "oldgamma:  1      pr_sd_theta:  2.157425225792545\n",
      "Iteration:  100\n",
      "count 21\n",
      "nitem:  0  with  3.8593755993853707\n",
      "nitem:  1  with  4.021705574638587\n",
      "nitem:  2  with  1.4563587144486367\n",
      "nitem:  3  with  -5.59671424349839\n",
      "oldgamma:  1      pr_sd_theta:  16.50768601942056\n",
      "Iteration:  200\n",
      "count 41\n",
      "nitem:  0  with  -1.1603885734927326\n",
      "nitem:  1  with  4.231196366139966\n",
      "nitem:  2  with  0.7527778666273348\n",
      "nitem:  3  with  -6.77103984921703\n",
      "oldgamma:  1      pr_sd_theta:  19.798168443823723\n",
      "Iteration:  300\n",
      "count 61\n",
      "nitem:  0  with  -1.7941148890949754\n",
      "nitem:  1  with  3.3316204153039854\n",
      "nitem:  2  with  0.9765690471059602\n",
      "nitem:  3  with  -10.479135224425596\n",
      "oldgamma:  1      pr_sd_theta:  22.304793192823585\n",
      "Iteration:  400\n",
      "count 81\n",
      "nitem:  0  with  2.3734707089639167\n",
      "nitem:  1  with  0.7416839825144441\n",
      "nitem:  2  with  -5.469356125040098\n",
      "nitem:  3  with  -11.530348101988668\n",
      "oldgamma:  1      pr_sd_theta:  28.588841404028503\n",
      "Iteration:  500\n",
      "count 101\n",
      "nitem:  0  with  -0.33520963533263226\n",
      "nitem:  1  with  2.2057555639009823\n",
      "nitem:  2  with  -0.3393823441905519\n",
      "nitem:  3  with  -10.18643340753019\n",
      "oldgamma:  1      pr_sd_theta:  26.09001693700091\n",
      "Iteration:  600\n",
      "count 121\n",
      "nitem:  0  with  -2.7307929653682494\n",
      "nitem:  1  with  1.6808170975943486\n",
      "nitem:  2  with  -3.6231688570530363\n",
      "nitem:  3  with  -9.369342159447966\n",
      "oldgamma:  1      pr_sd_theta:  29.37075952081565\n",
      "Iteration:  700\n",
      "count 141\n",
      "nitem:  0  with  0.8942232631344409\n",
      "nitem:  1  with  5.076247019590805\n",
      "nitem:  2  with  -6.40564359875411\n",
      "nitem:  3  with  -17.136397179014732\n",
      "oldgamma:  1      pr_sd_theta:  32.09419529178066\n",
      "Iteration:  800\n",
      "count 161\n",
      "nitem:  0  with  -1.708641705823767\n",
      "nitem:  1  with  7.629390415002582\n",
      "nitem:  2  with  -8.783155462717936\n",
      "nitem:  3  with  -16.489852113099985\n",
      "oldgamma:  1      pr_sd_theta:  34.204751071407934\n",
      "Iteration:  900\n",
      "count 181\n",
      "nitem:  0  with  -4.901189651167472\n",
      "nitem:  1  with  8.134803217851509\n",
      "nitem:  2  with  -9.191441198640646\n",
      "nitem:  3  with  -16.71501737233718\n",
      "oldgamma:  1      pr_sd_theta:  36.630463522251524\n"
     ]
    }
   ],
   "source": [
    "# Set 99 as missing\n",
    "\"\"\"\n",
    "output = onepl_lsrm_cont_missing(data,\n",
    "                                 ndim, niter, nburn, nthin, nprint,\n",
    "                                 jump_beta, jump_theta, jump_gamma, jump_z, jump_w,\n",
    "                                 pr_mean_beta, pr_sd_beta, pr_a_th_sigma, pr_b_th_sigma, pr_mean_theta,\n",
    "                                 pr_a_sigma, pr_b_sigma, pr_mean_gamma, pr_sd_gamma,\n",
    "                                 99)\n",
    "\"\"\"\n",
    "\n",
    "output = onepl_lsrm_cont_missing(logit_x,\n",
    "                                 ndim, niter, nburn, nthin, nprint,\n",
    "                                 jump_beta, jump_theta, jump_gamma, jump_z, jump_w,\n",
    "                                 pr_mean_beta, pr_sd_beta, pr_a_th_sigma, pr_b_th_sigma, pr_mean_theta,\n",
    "                                 pr_a_sigma, pr_b_sigma, pr_mean_gamma, pr_sd_gamma,\n",
    "                                 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1224b277-a592-46a5-8e4d-6a0e4c2dc702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f9f64ae-868d-448b-a0af-de3d5f894ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#output[\"accept_beta\"].T\n",
    "#output[\"accept_theta\"]\n",
    "### View(t(output$accept_theta))\n",
    "\n",
    "#output[\"accept_w\"].T\n",
    "#output[\"accept_gamma\"].T\n",
    "### View(t(output$accept_z))\n",
    "\n",
    "#output$accept_z\n",
    "##Rcode procrustes matching\n",
    "\"\"\" #TODO View Output Table?\n",
    "\n",
    "\n",
    "def procrustes_mine(X, X_star):\n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "\n",
    "    J = np.identity(n)\n",
    "\n",
    "    C = X_star.transpose() @ J @ X\n",
    "    svd_out = np.linalg.svd(C)\n",
    "    R = svd_out[3] @ svd_out[1].transpose\n",
    "    s = 1\n",
    "\n",
    "    tt = np.zeros((m, 1))\n",
    "\n",
    "    X.new = s * X @ R + tt.reshape(X.shape[0], X.shape[1])\n",
    "    return X.new\n",
    "\n",
    "# <editor-fold desc=\"MCMC Process\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "69457798-80f9-4a0d-8d93-c8e17b701c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-b830d78618a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnmcmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mniter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnburn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnthin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmax_address\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'map'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'map'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "nsample = data.shape[0]\n",
    "nitem  = data.shape[1]\n",
    "\n",
    "nmcmc = int((niter - nburn) / nthin)\n",
    "\n",
    "max_address = min(output['map'].index(max(output['map'])))\n",
    "\n",
    "w_star = output['w'].iloc[max_address, :, :]\n",
    "z_star = output['z'].iloc[max_address, :, :]\n",
    "\n",
    "w_proc = np.zeros((nmcmc, nitem, ndim), )\n",
    "z_proc = np.zeros((nmcmc, nsample, ndim), )\n",
    "\n",
    "for iter in range(nmcmc):\n",
    "    z_iter = output['z'].iloc[iter,:,:]\n",
    "\n",
    "    if iter != max_address:\n",
    "        z_proc[iter, :, :] = procrustes_mine(z_iter, z_star)\n",
    "        #z_proc[iter,:,:] = \"\"\"procrustes(z_iter, z_star)$X.new\"\"\" #TODO ======= 210717 =======\n",
    "    else: z_proc[iter,:,:] = z_iter\n",
    "\n",
    "    w_iter = output['w'].iloc[iter,:,:]\n",
    "\n",
    "    if iter != max_address:\n",
    "        w_proc[iter,:,:] = procrustes_mine(w_iter, w_star) #TODO ======= 210717 =======\n",
    "    else: w_proc[iter,:,:] = w_iter\n",
    "\n",
    "\n",
    "\n",
    "w_est = np.empty((nitem, ndim,))\n",
    "\n",
    "for i in range(nitem):\n",
    "    for j in range(ndim):\n",
    "        w_est[i, j] = w_proc[:, i, j].mean\n",
    "\n",
    "\n",
    "z_est = np.empty((nsample, ndim,))\n",
    "\n",
    "for k in range(nsample):\n",
    "    for j in range(ndim):\n",
    "        z_est[k, j] = z_proc[:, k, j].mean\n",
    "\n",
    "\n",
    "beta_est = output[\"beta\"].mean\n",
    "theta_est = output[\"theta\"].mean\n",
    "\n",
    "#beta_est = apply(output[\"beta\"], 2, mean)\n",
    "#theta_est = apply(output[\"theta\"], 2, mean)\n",
    "\n",
    "sigma_theta_est = output[\"sigma_theta\"].mean\n",
    "gamma_est = output[\"gamma\"].mean\n",
    "\n",
    "output_new = {\"beta_estimate\" : beta_est,\n",
    "              \"theta_estimate\" : theta_est,\n",
    "              \"sigma_theta_estimate\" : sigma_theta_est,\n",
    "              \"gamma_estimate\" : gamma_est,\n",
    "              \"z_estimate\" : z_est,\n",
    "              \"w_estimate\" : w_est,\n",
    "              \"beta\" : output[\"beta\"],\n",
    "              \"theta\" : output[\"theta\"],\n",
    "              \"theta_sd\" : output[\"sigma_theta\"],\n",
    "              \"gamma\" : output[\"gamma\"],\n",
    "              \"z\" : z_proc,\n",
    "              \"w\" : w_proc,\n",
    "              \"accept_beta\" : output[\"accept_beta\"],\n",
    "              \"accept_theta\" : output[\"accept_theta\"],\n",
    "              \"accept_w\" : output[\"accept_w\"],\n",
    "              \"accept_z\" : output[\"accept_z\"],\n",
    "              \"accept_gamma\" : output[\"accept_gamma\"]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e1d4ac9-f304-4947-9c93-f3b6853bfeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(output).keys()\n",
    "\n",
    "(output)[\"map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f67f1-90fb-45c8-a569-78061fad6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#t(output_new$accept_beta)\n",
    "#t(output_new$accept_theta)\n",
    "#t(output_new$accept_w)\n",
    "#t(output_new$accept_z)\n",
    "\n",
    "#output_new[\"accept_gamma\"]\n",
    "\n",
    "# save.image(\"full_continuous_60.RData\")\n",
    "# save.image(\"full_continuous_70.RData\")\n",
    "# save.image(\"full_continuous_80.RData\")\n",
    "# save.image(\"full_continuous_mean.RData\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "save.image(\"90_result.RData\")\n",
    "\n",
    "dir\n",
    "pdf(paste0(dir, \"/trace_beta.pdf\"))\n",
    "for (i in 1:ncol(output_new$beta)) ts.plot(output_new$beta[1:nrow(output_new$beta), i], main = paste(\"beta\", i))\n",
    "dev.off()\n",
    "\n",
    "pdf(paste0(dir, \"/trace_theta.pdf\"))\n",
    "for (i in 1:ncol(output_new$theta)) ts.plot(output_new$theta[, i], main=paste(\"theta\", i))\n",
    "dev.off()\n",
    "\n",
    "pdf(paste0(dir, \"/trace_sigma_theta.pdf\"))\n",
    "ts.plot(output_new$theta_sd, main = \"sigma_theta\")\n",
    "dev.off()\n",
    "\n",
    "pdf(paste0(dir, \"/trace_gamma.pdf\"))\n",
    "ts.plot(output_new$gamma, main = \"gamma\")\n",
    "dev.off()\n",
    "\"\"\"\n",
    "# </editor-fold>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir, \"/trace_z.pdf\"))\n",
    "for (k in 1:ncol(output_new$theta)) ts.plot(output_new$z[, k, 1], main=paste(\"z_1\", k))\n",
    "for (k in 1:ncol(output_new$theta)) ts.plot(output_new$z[, k, 2], main=paste(\"z_2\", k))\n",
    "dev.off()\n",
    "\n",
    "pdf(paste0(dir, \"/trace_w.pdf\"))\n",
    "for (i in 1:ncol(output_new$beta)) ts.plot(output_new$w[, i, 1], main=paste(\"w_1\", i))\n",
    "for (i in 1:ncol(output_new$beta)) ts.plot(output_new$w[, i, 2], main=paste(\"w_2\", i))\n",
    "dev.off()\n",
    "\"\"\" #TODO Multiple Write multiple Time Series Plot in pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d30954-c890-4ceb-be29-971890dbb3f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-98-732a16ee9ab6>, line 286)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-98-732a16ee9ab6>\"\u001b[1;36m, line \u001b[1;32m286\u001b[0m\n\u001b[1;33m    topic_plot = scatterplot3d(new[,1:3],\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "gg = ggplot() +\n",
    "    geom_text(data=bnew, aes(x=coordinate_1, y=coordinate_2, label=id), col=2) +\n",
    "    # geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = topic_name),col=2) +  #topic name\n",
    "    # geom_point(data = anew,aes(x=coordinate_1,y=coordinate_2),cex=1) +\n",
    "    xlim(min(bnew$coordinate_1, anew$coordinate_1)-0.2, max(bnew$coordinate_1, anew$coordinate_1)+0.2) +\n",
    "    ylim(min(bnew$coordinate_2, anew$coordinate_2)-0.2, max(bnew$coordinate_2, anew$coordinate_2)+0.2)\n",
    "    # xlim (-0.8,0.8) + ylim(-.8,.8)\n",
    "print(gg)\n",
    "\n",
    "b = bnew\n",
    "a = anew\n",
    "\n",
    "dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pdf(paste0(dir, \"/plot_wz.pdf\"))\n",
    "\n",
    "\n",
    "\n",
    "gg = ggplot() +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=id), col=2) +\n",
    "    # geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = topic_name),col=2) +  #topic name\n",
    "    geom_point(data=a, aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "    xlim(min(b$coordinate_1, a$coordinate_1)-0.2, max(b$coordinate_1, a$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a$coordinate_2)-0.2, max(b$coordinate_2, a$coordinate_2)+0.2)\n",
    "    # xlim (-0.8,0.8) + ylim(-.8,.8)\n",
    "print(gg)\n",
    "\n",
    "gg = ggplot() +\n",
    "    # geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = id),col=2) +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=topic_name), col=2) +  # topic name\n",
    "    geom_point(data=a, aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "    xlim(min(b$coordinate_1, a$coordinate_1)-0.2, max(b$coordinate_1, a$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a$coordinate_2)-0.2, max(b$coordinate_2, a$coordinate_2)+0.2)\n",
    "print(gg)\n",
    "\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir, \"/plot_wz_new.pdf\"))\n",
    "\n",
    "\n",
    "\n",
    "gg = ggplot() +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=id), col=2) +\n",
    "    # geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = topic_name),col=2) +  #topic name\n",
    "    geom_point(data=a_new, aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "    xlim(min(b$coordinate_1, a_new$coordinate_1)-0.2, max(b$coordinate_1, a_new$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a_new$coordinate_2)-0.2, max(b$coordinate_2, a_new$coordinate_2)+0.2)\n",
    "    # xlim (-0.8,0.8) + ylim(-.8,.8)\n",
    "print(gg)\n",
    "\n",
    "gg = ggplot() +\n",
    "    # geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = id),col=2) +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=topic_name), col=2) +  # topic name\n",
    "    geom_point(data=a_new, aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "    xlim(min(b$coordinate_1, a_new$coordinate_1)-0.2, max(b$coordinate_1, a_new$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a_new$coordinate_2)-0.2, max(b$coordinate_2, a_new$coordinate_2)+0.2)\n",
    "print(gg)\n",
    "\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir, \"/Plot5_3dplot_beta.pdf\"))\n",
    "\n",
    "\n",
    "\n",
    "topic_plot = scatterplot3d(new[, -4], pch = 16, color = colors, angle = 50)\n",
    "text(topic_plot$xyz.convert(new[, -4]+0.3), labels = new$topics)\n",
    "\n",
    "topic_plot = scatterplot3d(new[, -4], pch = 16, color = colors, angle = 50)\n",
    "text(topic_plot$xyz.convert(new[, -4]+0.3), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
    "                                                       19, 20))\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "# TODO =====================\n",
    "\n",
    "\"\"\"\n",
    "##plot of topic\n",
    "\n",
    "pdf(paste0(dir, \"/Plot1_topic_cluter.pdf\")) \n",
    "\n",
    "\n",
    "\n",
    "ggg = ggplot() +\n",
    "    geom_point(data=b, aes(x=coordinate_1, y=coordinate_2), col=0.5) +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=topic_name), col=as.factor(group), cex = 3) +\n",
    "    # geom_point(data = a, aes(x=coordinate_1,y= coordinate_2), cex=1) +\n",
    "    # geom_text(data = a[group_index,], aes(x=coordinate_1, y = coordinate_2+0.15, label = dbscan$cluster[group_index]),col=2) +\n",
    "    scale_color_manual(values=c(\"black\", brewer.pal(n=9, name='Set1'))) +\n",
    "    # geom_point(data = a[!dbscan$isseed,],aes(coordinate_1, coordinate_2), shape=8, cex=1) +\n",
    "    xlim(min(b$coordinate_1, a$coordinate_1)-0.2, max(b$coordinate_1, a$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a$coordinate_2)-0.2, max(b$coordinate_2, a$coordinate_2)+0.2) +\n",
    "    # xlim (-0.8,1.2) + ylim(-.8,.1) +\n",
    "    theme_bw() + theme(legend.position = \"None\")\n",
    "\n",
    "print(ggg)\n",
    "\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "word_position = pd.concat([data_word, a.iloc[:, 0:1]], axis=1)\n",
    "\n",
    "\n",
    "word_position[\"dist\"] = (word_position[\"coordinate_1\"]**2 + word_position[\"coordinate_2\"]**2)**0.5\n",
    "word_new = word_position.loc[word_position[\"dist\"] > 1.4, :]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "##plot of topic\n",
    "\n",
    "pdf(paste0(dir, \"/topic_cluter_withwords.pdf\"))\n",
    "\n",
    "\n",
    "ggg = ggplot() +\n",
    "    geom_point(data=b, aes(x=coordinate_1, y=coordinate_2), col=0.5) +\n",
    "    geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=topic_name), col=as.factor(group), cex = 3) +\n",
    "    # geom_point(data = a, aes(x=coordinate_1,y= coordinate_2), cex=1) +\n",
    "    # geom_text(data = a[group_index,], aes(x=coordinate_1, y = coordinate_2+0.15, label = dbscan$cluster[group_index]),col=2)+\n",
    "    scale_color_manual(values=c(\"black\", brewer.pal(n=9, name='Set1'))) +\n",
    "    # geom_point(data = a[!dbscan$isseed,],aes(coordinate_1, coordinate_2), shape=8, cex=1) +\n",
    "    geom_point(data=a_new, aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "    geom_label_repel(data=word_new, aes(x=coordinate_1, y=coordinate_2, label=data_word)) +\n",
    "    xlim(min(b$coordinate_1, a_new$coordinate_1)-0.2, max(b$coordinate_1, a_new$coordinate_1)+0.2) +\n",
    "    ylim(min(b$coordinate_2, a_new$coordinate_2)-0.2, max(b$coordinate_2, a_new$coordinate_2)+0.2) +\n",
    "    theme_bw() + theme(legend.position = \"None\")\n",
    "print(ggg)\n",
    "\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "quantile(word_position$dist, c(0.05, 0.2))\n",
    "word_new = word_position[word_position[\"dist\"] < 0.41,]\n",
    "\n",
    "#word_new\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir,\"/Plot4_topic_cluter_centerwords.pdf\"))\n",
    "\n",
    "##plot of topic \n",
    "ggg = ggplot() +\n",
    "  geom_point(data = b, aes(x=coordinate_1, y = coordinate_2),col=0.5) +\n",
    "  geom_text(data = b, aes(x=coordinate_1, y = coordinate_2, label = colnames(data_set2)),col=as.factor(group), cex=3) +\n",
    "  # geom_point(data = a, aes(x=coordinate_1,y= coordinate_2), cex=1) +\n",
    "  # geom_text(data = a[group_index,], aes(x=coordinate_1, y = coordinate_2+0.15, label = dbscan$cluster[group_index]),col=2)+\n",
    "  scale_color_manual(values = c(\"black\", brewer.pal(n = 9, name = 'Set1'))) +\n",
    "  # geom_point(data = a[!dbscan$isseed,],aes(coordinate_1, coordinate_2), shape=8, cex=1) +\n",
    "  geom_point(data = a_new,aes(x=coordinate_1,y=coordinate_2),cex=1) + \n",
    "  geom_label_repel(data = word_new, aes(x=coordinate_1,y=coordinate_2, label = data_word))+\n",
    "  xlim (min(b$coordinate_1,a_new$coordinate_1)-0.2,max(b$coordinate_1,a_new$coordinate_1)+0.2) + \n",
    "  ylim(min(b$coordinate_2,a_new$coordinate_2)-0.2,max(b$coordinate_2,a_new$coordinate_2)+0.2) +\n",
    "  theme_bw() + theme(legend.position = \"None\") \n",
    "print(ggg)\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "words = data_word\n",
    "close_word_index = []\n",
    "\n",
    "## cosine similarity\n",
    "\n",
    "temp = pd.concat([b.iloc[:, 1:2], group], axis = 1)\n",
    "#head(temp)\n",
    "\n",
    "## center of topic group\n",
    "\n",
    "\n",
    "#temp2 = data.frame(temp % > % group_by(group) % > % summarise(x=mean(coordinate_1), y=mean(coordinate_2)))\n",
    "\n",
    "temp2 = temp.groupby(\"group\", axis=1).agg({\"coordinate_1\":\"mean\", \"coordinate_2\":\"mean\"})\n",
    "temp2.columns = [\"x\", \"y\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#head(temp2)\n",
    "\n",
    "word_position = pd.concat([words, a[, 1:2]], axis = 1)\n",
    "close_word_index = []\n",
    "#head(word_position)\n",
    "\n",
    "library(ggrepel)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir, \"/Plot3_topic_close_words.pdf\"))\n",
    "\n",
    "for (i in 1:ncol(data_set2)){z\n",
    "    ## here 10\n",
    "    # i = 1\n",
    "    close_word_index[[i]] = order(as.matrix(dist(rbind(b[i, 1:2], a[, 1:2])))[1, -1])[1: 25]\n",
    "    ggg = ggplot() +\n",
    "        geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=seq(1: ncol(data_set2))), col =as.factor(group), cex = 3) +\n",
    "        geom_point(data=word_position[close_word_index[[i]],], aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "        geom_point(data=b[i,], aes(x=coordinate_1, y=coordinate_2), col='red', cex=7, shape=2, stroke=1) +\n",
    "        scale_color_manual(values=c(\"black\", brewer.pal(n=9, name='Set1'))) +\n",
    "        xlim(min(b$coordinate_1, a$coordinate_1)-0.5, max(b$coordinate_1, a$coordinate_1)+0.5) +\n",
    "        ylim(min(b$coordinate_2, a$coordinate_2)-0.5, max(b$coordinate_2, a$coordinate_2)+0.5) +\n",
    "        labs(title=paste(\"Topic\", i), x=\"\", y=\"\") +\n",
    "        theme_bw() + theme(legend.position = \"None\")\n",
    "    \n",
    "    # ggg <- ggg+ geom_text_repel(data = word_position[close_word_index,], aes(x=X1, y = X2, label = covid_20_word), size=3)\n",
    "\n",
    "    ggg1 = ggg + \n",
    "        geom_label_repel(data=word_position[close_word_index[[i]],],\n",
    "                         aes(x=coordinate_1, y=coordinate_2, label=words),\n",
    "                         # fontface = 'bold', color = 'white', box.padding = unit(0.5, \"lines\"),\n",
    "                         point.padding = unit(0.5, \"lines\"),\n",
    "                         segment.color = 'grey50'\n",
    "                        )\n",
    "    print(ggg1)\n",
    "}\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "##### Cluster_Group near words\n",
    "\n",
    "def euc_dist(x1, x2): sqrt(sum((x1 - x2)**2))\n",
    "\n",
    "#head(word_position)\n",
    "\n",
    "\"\"\"\n",
    "pdf(paste0(dir, \"/Plot2_cluster_close_words.pdf\"))\n",
    "\n",
    "\n",
    "\n",
    "for (i in 1:nrow(temp2)){\n",
    "    # i = 1\n",
    "    ## here 10\n",
    "    cluster_word_dist < -c()\n",
    "    close_word_index < -c()\n",
    "    \n",
    "    for (k in c(1:nrow(output_new$z_estimate))){\n",
    "        cluster_word_dist[k]=euc.dist(temp2[i, -1], output_new$z_estimate[k, ])\n",
    "    }\n",
    "    close_word_index < -order(as.matrix(cluster_word_dist))[1: 25]\n",
    "    ggg = ggplot() +\n",
    "        geom_text(data=b, aes(x=coordinate_1, y=coordinate_2, label=seq(1: ncol(data_set2))), col =as.factor(group), cex = 3) +\n",
    "        geom_point(data=word_position[close_word_index,], aes(x=coordinate_1, y=coordinate_2), cex=1) +\n",
    "        geom_text(data=temp2[, -1], aes(x=x, y=y, label=paste(\"Group\", seq(1, nrow(temp2)), sep=\"\")), col = \"red\", cex = 3) +\n",
    "        # geom_point(data = b[i,], aes(x=coordinate_1,y= coordinate_2), col='red', cex=7, shape=2, stroke = 1) +\n",
    "        scale_color_manual(values=c(\"black\", brewer.pal(n=9, name='Set1'))) +\n",
    "        labs(title=paste(\"Cluster\", i), x=\"\", y=\"\") +\n",
    "        theme_bw() + theme(legend.position = \"None\")\n",
    "\n",
    "    print(ggg)\n",
    "    ggg1 = ggg + geom_label_repel(data=word_position[close_word_index,], \n",
    "                                    aes(x=coordinate_1, y=coordinate_2, label=words),\n",
    "                                    # fontface = 'bold', color = 'white',\n",
    "                                    box.padding = unit(0.5, \"lines\"),\n",
    "                                    point.padding = unit(0.5, \"lines\"),\n",
    "                                    segment.color = 'grey50'\n",
    "                                )\n",
    "    print(ggg1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dev.off()\n",
    "\"\"\"\n",
    "\n",
    "save.image(\"75_result.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866e8a-e3e0-4123-bc4a-6afe1874423b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
